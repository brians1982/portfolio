{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f406e46c",
   "metadata": {},
   "source": [
    "### Brian Shula, 2024\n",
    "This project creates a Physics Informed Neural Network to approximate the time-varying temperature distribution in a fin as it cools due to convection and radiation from an initial temperature.  The neural network is trained with sparse data from a finite element model (ground truth), and employs the governing partial differential equations to calculate Loss from model residuals.  A schematic of the problem is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e465a71",
   "metadata": {},
   "source": [
    "![Fin Setup](Fin_Setup.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051a6559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "torch.manual_seed(47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7307173-991d-473e-acdf-a68912dc88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bd2ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whether to scale inputs and outputs to neural network\n",
    "scale = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1930f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEA Data - Ground Truth coordinates, time, and temperature\n",
    "df_data = pd.read_csv('Nodal_Temperatures.csv')\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d2bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = np.array(df_data[['x','y','time','temperature']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6146d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random shuffle - shuffle data pairs for splitting\n",
    "np.random.seed(23)\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "rng.shuffle(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92326bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cef6d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data set - subset of total FEA data\n",
    "ndata = model_data.shape[0]//10\n",
    "\n",
    "#Test/Train Split\n",
    "xyt_train = model_data[:ndata, 0:3]\n",
    "xyt_test = model_data[ndata:, 0:3]\n",
    "\n",
    "T_train = model_data[:ndata, 3]\n",
    "T_test = model_data[ndata:, 3]\n",
    "\n",
    "#Scale\n",
    "xyt_train_scaler = MinMaxScaler()\n",
    "xyt_train = xyt_train_scaler.fit_transform(xyt_train)\n",
    "xyt_test = xyt_train_scaler.transform(xyt_test)\n",
    "\n",
    "#Reshape and Scale\n",
    "T_train = T_train.reshape((-1,1))\n",
    "T_test = T_test.reshape((-1,1))\n",
    "\n",
    "\n",
    "#Define scaler but don't fit - Loss calculated in physical domain\n",
    "T_train_scaler = MinMaxScaler()\n",
    "T_train_scaler.fit(T_train)\n",
    "    \n",
    "#Temperature Scaler quantities: Tscaled = (T - Tmin) / Trange\n",
    "Trange = T_train_scaler.data_range_[0]\n",
    "Tmin = T_train_scaler.data_min_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dcf99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Inputs, Outputs to Torch tensors\n",
    "xyt_train_pt = torch.tensor(xyt_train, dtype=torch.float32).to(device)\n",
    "xyt_test_pt = torch.tensor(xyt_test, dtype=torch.float32).to(device)\n",
    "\n",
    "T_train_pt = torch.tensor(T_train, dtype=torch.float32).to(device)\n",
    "T_test_pt = torch.tensor(T_test, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce6a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heat Transfer Constants\n",
    "k = 186 #Thermal conductivity, W/m*K\n",
    "cp = 875 #Specific Heat, J/kg*K\n",
    "rho = 2770 #Density, kg/m^3\n",
    "\n",
    "SB = 5.67e-8 #Stefan-Boltzmann constant W/m^2*K^4\n",
    "eps = 0.8 #Emissivity\n",
    "film_coeff = 50 #Conection film coefficient, W/m^2*K\n",
    "\n",
    "T_inf = 25 #Degrees C, surrounding temperature\n",
    "T_init = 250 #Degrees C\n",
    "T_bc = 250 #Degrees C\n",
    "\n",
    "#Model information\n",
    "xmin = 0 #meters\n",
    "ymin = 0 #meters\n",
    "\n",
    "xmax = 0.05 #meters\n",
    "ymax = 0.25 #meters\n",
    "\n",
    "tmin = 0 #seconds\n",
    "tmax = 3600 #seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38caff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collocation Discretization\n",
    "#Place points inside block\n",
    "xybuffer = .002\n",
    "tbuffer = 10\n",
    "\n",
    "ncoll_pts = 20\n",
    "\n",
    "xdims = np.linspace(xmin + xybuffer, xmax - xybuffer, ncoll_pts)\n",
    "ydims = np.linspace(ymin + xybuffer, ymax - xybuffer, 3 * ncoll_pts)\n",
    "tsteps = np.linspace(tmin + tbuffer, tmax - tbuffer, 6 * ncoll_pts)\n",
    "\n",
    "xx, yy, tt = np.meshgrid(xdims, ydims, tsteps)\n",
    "\n",
    "xx = xx.reshape((-1,1))\n",
    "yy = yy.reshape((-1,1))\n",
    "tt = tt.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cd2fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group and convert to PyTorch variable\n",
    "xyt_coll = np.concatenate((xx, yy, tt), axis=1)\n",
    "\n",
    "#Shuffle\n",
    "rng.shuffle(xyt_coll)\n",
    "\n",
    "#Scale\n",
    "xyt_coll = xyt_train_scaler.transform(xyt_coll)\n",
    "\n",
    "#Convert to PyTorch tensor\n",
    "xyt_coll_pt = torch.tensor(xyt_coll, dtype=torch.float32, requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed913d5",
   "metadata": {},
   "source": [
    "# PINN Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0ab06b",
   "metadata": {},
   "source": [
    "## Physics Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75721ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def physics_loss(pts, net):\n",
    "    \n",
    "    #Calculate loss of d**2T/dx**2 + d**2T/dy**2 = dT/dt\n",
    "    \n",
    "    #Predict temperatures\n",
    "    T_pred = net(pts)\n",
    "\n",
    "    #Scale back to physical temperatures - distances scaled separately\n",
    "    T_pred = T_pred * Trange + Tmin\n",
    "\n",
    "    #Calculate gradients\n",
    "    dTdx = torch.autograd.grad(T_pred, pts, grad_outputs=torch.ones_like(T_pred), create_graph=True, retain_graph=True)[0][:,0]\n",
    "    dTdy = torch.autograd.grad(T_pred, pts, grad_outputs=torch.ones_like(T_pred), create_graph=True, retain_graph=True)[0][:,1]\n",
    "    \n",
    "    #Scale distances\n",
    "    dTdx = dTdx / xmax\n",
    "    dTdy = dTdy / ymax\n",
    "    \n",
    "    d2Tdx2 = torch.autograd.grad(dTdx, pts, grad_outputs=torch.ones_like(dTdx), create_graph=True, retain_graph=True)[0][:,0]\n",
    "    d2Tdy2 = torch.autograd.grad(dTdy, pts, grad_outputs=torch.ones_like(dTdy), create_graph=True, retain_graph=True)[0][:,1]\n",
    "    \n",
    "    #Scale distances\n",
    "    d2Tdx2 = d2Tdx2 / xmax\n",
    "    d2Tdy2 = d2Tdy2 / ymax\n",
    "    \n",
    "    \n",
    "    dTdt = torch.autograd.grad(T_pred, pts, grad_outputs=torch.ones_like(T_pred), create_graph=True, retain_graph=True)[0][:,2]\n",
    "    #Scale distance\n",
    "    dTdt = dTdt / tmax\n",
    "    \n",
    "    alpha = k / (rho * cp)\n",
    "\n",
    "    #Residuals\n",
    "    res = alpha * (d2Tdx2 + d2Tdy2) - dTdt\n",
    "\n",
    "    loss = torch.mean((res)**2)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66f4803",
   "metadata": {},
   "source": [
    "## Convection/Radiation Loss\n",
    "The Convection and Radiation heat transfer rates must match the conduction heat transfer rate.  The conduction rate may be calcuated by a temperature derivative in the material.  In order to do this, two layers of Collocation points are generated to allow the gradient calculation. A set of points is generated for each free edge and its respective gradient.  This is shown schematically:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce46956d",
   "metadata": {},
   "source": [
    "![Collocation Points](Collocation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce05907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convection(xy, yx, side, net):\n",
    "    '''\n",
    "    Function that evaluates radiation+convection/conduction interface loss in a general sense - \n",
    "    collocation pts are generated based on inputs for each edge:\n",
    "    xy: int. - x or y represented as 1 or 2 - edge normal\n",
    "    yx: float - defines location of edge normal/ edge\n",
    "    side: int. - defines solid material side of boundary in global sys: +/-1\n",
    "    '''\n",
    "    \n",
    "    #Define point spacing\n",
    "    nbc_pts = 15\n",
    "    tbuffer = 1. #Avoid interferring w/ t=0 bc\n",
    "    times = np.linspace(tmin + tbuffer, tmax, 3 * nbc_pts)\n",
    "\n",
    "    #Create points along edge\n",
    "    if xy == 1:\n",
    "        #Move BC slightly above bottom edge Temp.=250C BC\n",
    "        pts_edge = np.linspace(ymin + 0.005, ymax, nbc_pts)\n",
    "        width = xmax - xmin\n",
    "        npts = pts_edge.shape[0]\n",
    "    elif xy == 2:\n",
    "        pts_edge = np.linspace(xmin, xmax, nbc_pts)\n",
    "        width = ymax - ymin\n",
    "        npts = pts_edge.shape[0]\n",
    "\n",
    "    #Create meshgrid of varied pairs\n",
    "    pts_edge, times = np.meshgrid(pts_edge, times)\n",
    "\n",
    "    #Reshape varied dimensions\n",
    "    pts_edge = pts_edge.reshape((-1,1))\n",
    "    times = times.reshape((-1,1))   \n",
    "\n",
    "    #Create constant dim along edge and just interior to allow gradient calculation\n",
    "    pts_norm_edge = np.zeros_like(pts_edge) + yx\n",
    "    pts_norm_interior = pts_norm_edge + side * 0.01 * width\n",
    "\n",
    "    if xy == 1:\n",
    "        pts1 = np.concatenate((pts_norm_edge, pts_edge, times), axis=1)\n",
    "        pts2 = np.concatenate((pts_norm_interior, pts_edge, times), axis=1)\n",
    "        pts = np.concatenate((pts1, pts2), axis=0)\n",
    "    elif xy == 2:\n",
    "        pts1 = np.concatenate((pts_edge, pts_norm_edge, times), axis=1)\n",
    "        pts2 = np.concatenate((pts_edge, pts_norm_interior, times), axis=1)\n",
    "        pts = np.concatenate((pts1, pts2), axis=0)\n",
    "\n",
    "    #Scale inputs\n",
    "    pts = xyt_train_scaler.transform(pts)\n",
    "    \n",
    "    #Create PyTorch tensor\n",
    "    conv_pts_pt = torch.tensor(pts, dtype=torch.float32, requires_grad=True).to(device)\n",
    "\n",
    "    #Predictions\n",
    "    T_pred = net(conv_pts_pt)\n",
    "    \n",
    "    if scale:\n",
    "        #Scale back to physical temperatures - distances scaled separately\n",
    "        T_pred = T_pred * Trange + Tmin\n",
    "\n",
    "    if xy == 1:\n",
    "        #gradient parallel to x-axis: y-dim --> [:npts,1] index on exterior pts\n",
    "        grad = torch.autograd.grad(T_pred, conv_pts_pt, grad_outputs=torch.ones_like(T_pred), \n",
    "                                   create_graph=True, retain_graph=True)[0][:npts,0]\n",
    "        #Scale distance\n",
    "        grad = grad / xmax\n",
    "\n",
    "    elif xy == 2:\n",
    "        #gradient parallel to y-axis: y-dim --> [:npts,2] index on exterior pts\n",
    "        grad = torch.autograd.grad(T_pred, conv_pts_pt, grad_outputs=torch.ones_like(T_pred), \n",
    "                                   create_graph=True, retain_graph=True)[0][:npts,1]\n",
    "        #Scale distance\n",
    "        grad = grad / ymax\n",
    "\n",
    "\n",
    "    #Convert to Kelvin\n",
    "    T_pred_K = T_pred + 273\n",
    "    T_inf_K = T_inf + 273\n",
    "\n",
    "    #Residual: k*dT/dx = h(Tinf - Twall) + eps * sigma * (Tinf**4-T**4)\n",
    "\n",
    "    #Divide by cp because these are in Joules, making Loss very sensitive\n",
    "    cond = k * grad / cp\n",
    "    \n",
    "    conv = -side * film_coeff * (T_pred - T_inf) / cp\n",
    "    rad = -side * eps * SB * (T_pred_K**4 - T_inf_K**4) / cp\n",
    "\n",
    "    loss = torch.mean((cond + conv + rad)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e895f7",
   "metadata": {},
   "source": [
    "## Neural Network setup\n",
    "The PyTorch PINN has several hidden layers, with more neurons in the first layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14668db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model\n",
    "#Base Number of Neurons in hidden layers\n",
    "n_neurons=32\n",
    "\n",
    "class Surrogate(nn.Module):\n",
    "    def __init__(self,input_dim, output_dim):\n",
    "        super(Surrogate,self).__init__()  \n",
    "        #Fully connected layers\n",
    "        self.fc1=nn.Linear(input_dim,4*n_neurons)\n",
    "        self.fc2=nn.Linear(4*n_neurons,2*n_neurons)\n",
    "        self.fc3=nn.Linear(2*n_neurons,n_neurons)\n",
    "        self.fc4=nn.Linear(n_neurons,n_neurons)\n",
    "        self.fc5=nn.Linear(n_neurons,output_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #Construct layers\n",
    "        x=self.fc1(x)\n",
    "        x=torch.tanh(x)  \n",
    "        \n",
    "        x=self.fc2(x)\n",
    "        x=torch.tanh(x)  \n",
    "\n",
    "        x=self.fc3(x)\n",
    "        x=torch.tanh(x) \n",
    "        \n",
    "        x=self.fc4(x)\n",
    "        x=torch.tanh(x)\n",
    "        \n",
    "        x=self.fc5(x)\n",
    "                     \n",
    "        return x                               \n",
    "\n",
    "\n",
    "#Create model    \n",
    "input_dim = xyt_train.shape[1]\n",
    "output_dim = T_train.shape[1]\n",
    "\n",
    "torch.manual_seed(41)\n",
    "model=Surrogate(input_dim, output_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca5e28c",
   "metadata": {},
   "source": [
    "## Boundary and Initial Condition data setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc1ba02",
   "metadata": {},
   "source": [
    "#### The bottom edge is held at 250 deg. C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a94e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boundary Conditions data preparation\n",
    "# [x, y=0, t] = 250\n",
    "\n",
    "nbc_pts1 = 15\n",
    "xbc1 = np.linspace(xmin, xmax, nbc_pts1)\n",
    "tbc1 = np.linspace(tmin, tmax, nbc_pts1)\n",
    "\n",
    "xbc1, tbc1 = np.meshgrid(xbc1, tbc1)\n",
    "ybc1 = np.zeros_like(xbc1)\n",
    "\n",
    "xbc1 = xbc1.reshape((-1,1))\n",
    "ybc1 = ybc1.reshape((-1,1))\n",
    "tbc1 = tbc1.reshape((-1,1))\n",
    "\n",
    "xyt_bc1 = np.concatenate((xbc1, ybc1, tbc1), axis=1)\n",
    "T_true_bc1 = np.ones_like(tbc1) * T_bc\n",
    "\n",
    "#Scale inputs\n",
    "xyt_bc1 = xyt_train_scaler.transform(xyt_bc1)\n",
    "\n",
    "xyt_bc1_pt = torch.tensor(xyt_bc1, dtype=torch.float32).to(device)\n",
    "T_true_bc1_pt = torch.tensor(T_true_bc1, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c44391",
   "metadata": {},
   "source": [
    "#### Initial temperatures everywhere are 250 deg. C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f10ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Conditions 2 data preparation\n",
    "# [x, y, t=0] = 250\n",
    "\n",
    "nbc_pts2 = 15\n",
    "xbc2 = np.linspace(xmin, xmax, nbc_pts2)\n",
    "ybc2 = np.linspace(ymin, ymax, 3 * nbc_pts2)\n",
    "\n",
    "xbc2, ybc2 = np.meshgrid(xbc2, ybc2)\n",
    "\n",
    "tbc2 = np.zeros_like(ybc2)\n",
    "\n",
    "xbc2 = xbc2.reshape((-1,1))\n",
    "ybc2 = ybc2.reshape((-1,1))\n",
    "tbc2 = tbc2.reshape((-1,1))\n",
    "\n",
    "xyt_bc2 = np.concatenate((xbc2, ybc2, tbc2), axis=1)\n",
    "T_true_bc2 = np.ones_like(tbc2) * T_bc\n",
    "\n",
    "#Scale\n",
    "xyt_bc2 = xyt_train_scaler.transform(xyt_bc2)\n",
    "\n",
    "#Convert to PyTorch Tensors\n",
    "xyt_bc2_pt = torch.tensor(xyt_bc2, dtype=torch.float32).to(device)\n",
    "T_true_bc2_pt = torch.tensor(T_true_bc2, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf12bdce",
   "metadata": {},
   "source": [
    "##  Optimizer and Supporting Components\n",
    "The ADAM optimizer is used with learning rate decay.  The Cosine Annealing function decreases the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4cb3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE Loss\n",
    "MSE = nn.MSELoss()\n",
    "\n",
    "#Optimizer and Learning Rate Decay\n",
    "optimizer=optim.Adam(params = model.parameters(), lr = 0.001)\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, \n",
    "                             T_max = 1000, # Maximum number of iterations.\n",
    "                             eta_min = .0001) # Minimum learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f33b76",
   "metadata": {},
   "source": [
    "## Training \n",
    "Training Data and Collocation Points are split into batches during the training loop. The Total Loss is printed at intervals to monitor progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7923f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model:\n",
    "loss_history=[]\n",
    "\n",
    "#Manual batch size\n",
    "nbatches = 10\n",
    "dbatch_size = len(T_train_pt)//nbatches\n",
    "pbatch_size = len(xyt_coll_pt)//nbatches\n",
    "starts_data = np.arange(0, len(T_train_pt), dbatch_size)\n",
    "starts_phys = np.arange(0, len(xyt_coll_pt), pbatch_size)\n",
    "\n",
    "num_of_epochs=200\n",
    "for i in range(num_of_epochs):\n",
    "\n",
    "    #Reset tracked loss\n",
    "    tracked_loss = 0\n",
    "    \n",
    "    #loop over batches\n",
    "    for start_data, start_physics in zip(starts_data, starts_phys):\n",
    "        xyt_train_pt_batch = xyt_train_pt[start_data:start_data+dbatch_size]\n",
    "        T_train_pt_batch = T_train_pt[start_data:start_data+dbatch_size]\n",
    "        \n",
    "        xyt_coll_pt_batch = xyt_coll_pt[start_physics:start_physics+pbatch_size]\n",
    "\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        #Data Loss\n",
    "        T_pred = model(xyt_train_pt_batch) \n",
    "        if scale:\n",
    "            T_pred = T_pred * Trange + Tmin\n",
    "        data_loss = torch.mean((T_pred - T_train_pt_batch)**2)\n",
    "\n",
    "        #BC1: Bottom edge = 250\n",
    "        T_net_bc1 = model(xyt_bc1_pt)\n",
    "        if scale:\n",
    "            T_net_bc1 = T_net_bc1 * Trange + Tmin\n",
    "        BC_loss1 = torch.mean((T_net_bc1 - T_true_bc1_pt)**2)\n",
    "        \n",
    "        #BC2: T0 = 250\n",
    "        T_net_bc2 = model(xyt_bc2_pt)\n",
    "        if scale:\n",
    "            T_net_bc2 = T_net_bc2 * Trange + Tmin\n",
    "        BC_loss2 = torch.mean((T_net_bc2 - T_true_bc2_pt)**2)\n",
    " \n",
    "        #Physics Loss\n",
    "        PINN_loss = physics_loss(xyt_coll_pt_batch, model)\n",
    "        \n",
    "        #Physics loss - convection\n",
    "        #convection(xy, yx, side, net) - normal, edge coord, material side\n",
    "        #Weight losses by side length\n",
    "        #Length of conv. BC: 2*sides + top\n",
    "        conv_length = 2 * (ymax - ymin) + (xmax - xmin)\n",
    "        \n",
    "        lhs_loss = convection(1, 0, 1, model) * (ymax - ymin) / conv_length\n",
    "        top_loss = convection(2, .25, -1, model) * (xmax - xmin) / conv_length\n",
    "        rhs_loss = convection(1, .05, -1, model) * (ymax - ymin) / conv_length\n",
    "        \n",
    "        conv_loss = lhs_loss + top_loss + rhs_loss\n",
    "        \n",
    "\n",
    "        total_loss = data_loss + PINN_loss + 0.2*conv_loss + BC_loss1 + BC_loss2\n",
    "\n",
    "\n",
    "\n",
    "        # back prop\n",
    "        total_loss.backward()  \n",
    "\n",
    "        # update weights and learning rate\n",
    "        optimizer.step()\n",
    "        \n",
    "        tracked_loss = tracked_loss + total_loss.cpu().detach().numpy()\n",
    "    \n",
    "    #Record Loss on last batch\n",
    "    loss_history.append(tracked_loss)    \n",
    "    \n",
    "    #Update Learning Rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # print the loss in training part:\n",
    "    if i % 25 == 0:\n",
    "        learning_rate = optimizer.param_groups[0][\"lr\"]\n",
    "        print(f'Epoch: {i}: Loss={tracked_loss}, Learning Rate:{learning_rate:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6832c3",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "The PINN predictions are compared to FEA data for a specified time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a9e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_time = 510\n",
    "eval_df = df_data[df_data['time'] == eval_time]\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ed014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull Data from FEA dataframe\n",
    "eval_in = np.array(eval_df[['x', 'y', 'time']])\n",
    "eval_out = np.array(eval_df[['temperature']])\n",
    "\n",
    "#Reshape for plotting\n",
    "eval_out = eval_out.reshape(51,11)\n",
    "\n",
    "#Scale inputs and convert to tensor\n",
    "eval_in = xyt_train_scaler.transform(eval_in)\n",
    "eval_in_pt = torch.tensor(eval_in, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28836ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch model evaluation mode:\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    #Evaluate Neural Network with test points\n",
    "    T_eval = model(eval_in_pt)\n",
    "    T_eval = T_eval.cpu().detach().numpy()\n",
    "    #Inverse scaling\n",
    "    T_eval = T_train_scaler.inverse_transform(T_eval)\n",
    "    #Reshape for plotting\n",
    "    T = T_eval.reshape(51,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8e5252",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(6, 6), layout='constrained')\n",
    "\n",
    "fig.suptitle('PINN Predictions and Ground Truth at Time = ' + str(eval_time) + 's',\n",
    "            fontsize=14, fontweight='bold')\n",
    "\n",
    "im0 = ax0.imshow(T, origin='lower', cmap='jet', extent=[0, 0.05, 0, 0.25])\n",
    "plt.colorbar(im0, ax=ax0)\n",
    "ax0.set_title('PINN Predictions')\n",
    "ax0.set_xlabel('x (m)')\n",
    "ax0.set_ylabel('y (m)')\n",
    "\n",
    "#FEA Ground Truth\n",
    "im1 = ax1.imshow(eval_out, origin='lower', cmap='jet', extent=[0, 0.05, 0, 0.25])\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "ax1.set_title('Ground Truth')\n",
    "ax1.set_xlabel('x (m)')\n",
    "ax1.set_ylabel('y (m)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd131728",
   "metadata": {},
   "source": [
    "### Contour Plot saved from training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f0fd0",
   "metadata": {},
   "source": [
    "![Contour Plot](contours.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275207f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
